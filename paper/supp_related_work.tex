\section{Related Work}
\label{sec:supp-related-work}

\textbf{Adversarial Examples and Defenses:} Adversarial examples, first reported in \cite{SzegedyARXIV2013}, can be generated using a wide-range of white-box attacks \cite{SzegedyARXIV2013,GoodfellowARXIV2014,KurakinARXIV2016b,PapernotSP2016b,MoosaviCVPR2016,MadryARXIV2017,CarliniSP2017,DongCVPR2018,LuoAAAI2018}, with full access to the network, or black-box attacks \cite{ChenAISEC2017,BrendelARXIV2017a,SuARXV2017,IlyasARXIV2018,SarkarARXIV2017,NarodytskaCVPRWORK2017}, with limited access to model queries.
Besides certified and provable defenses \cite{CohenARXIV2019,YangARXIV2020,KumarARXIV2020b,ZhangNIPS2018,ZhangARXIV2019,WongICML2018,GowalARXIV2019,GehrSP2018,MirmanICML2018,SinghNIPS2018,LeeARXIV2019,CroceARXIV2018}, adversarial training (AT) has become the de-facto standard, as discussed in the main paper. However, there are also many detection/rejection approaches \cite{GrosseARXIV2017,FeinmanARXIV2017,LiaoCVPR2018,MaARXIV2018,AmsalegWIFS2017,MetzenARXIV2017}, so-called manifold-projection methods \cite{IlyasARXIV2017,SamangoueiICLR2018,SchottARXIV2018,ShenARXIV2017}, several methods based on pre-processing, quantization and/or dimensionality reduction \cite{BuckmanICLR2018,PrakashDCC2018,BhagojiARXIV2017}, methods based on randomness, regularization or adapted architectures \cite{ZantedschiAISEC2017,BhagojiARXIV2017,NayebiARXIV2017,SimonGabrielARXIV2018,HeinNIPS2017,JakubovitzARXIV2018,RossAAAI2018,KannanARXIV2018,LambARXIV2018,XieICLR2018} or ensemble methods \cite{LiuARXIV2017,StraussARXIV2017,HeUSENIXWORK2017,TramerICLR2018}, to name a few directions. However, often these defenses can be broken by considering adaptive attacks \cite{CarliniAISec2017,CarliniARXIV2016,AthalyeARXIV2018b,AthalyeARXIV2018}.

\textbf{Weight Robustness:} Flatness, \wrt the clean or robust loss surface, is also related to robustness in the weights. However, only few works explicitly study this ``weight robustness'': \cite{WengAAAI2020} considers robustness \wrt $L_\infty$ weight perturbations, while \cite{CheneyARXIV2017} studies Gaussian noise on weights. \cite{RakinICCV2019,HeCVPR2020}, in contrast, adversarially flip bits in (quantized) weights to reduce performance. Recently, \cite{StutzMLSYS2021} shows that robustness in weights can improve energy efficiency of neural network accelerators (\ie, specialized hardware for inference). This type of weight robustness is also relevant for some backdooring attacks that explicitly manipulate weights \cite{JiCCS2018,DumfordARXIV2018}. Fault tolerance is also a related concept, as it often involved changes in units or weights. It has been studied in early works \cite{NetiTNN1992,Chiu1994,DeodhareTNN1998}, obtaining fault tolerance using approaches similar to adversarial training NNs using approaches similar to adversarial training. However, there are also more recent works, \eg, weight dropping regularization \cite{RahmanICIP2018} or GAN-based training \cite{DudduARXIV2019}. We refer to \cite{TorreshuitzilIEEEACCESS2017} for a comprehensive survey.